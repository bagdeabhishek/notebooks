{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bagdeabhishek/notebooks/blob/master/MarathiLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "### Text Completion / Raw Text Training\n",
        "This is a community notebook collaboration with [Mithex].\n",
        "\n",
        "We train on `Tiny Stories` (link [here](https://huggingface.co/datasets/roneneldan/TinyStories)) which is a collection of small stories. For example:\n",
        "```\n",
        "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun.\n",
        "Beep was a healthy car because he always had good fuel....\n",
        "```\n",
        "Instead of `Alpaca`'s Question Answer format, one only needs 1 column - the `\"text\"` column. This means you can finetune on any dataset and let your model act as a text completion model, like for novel writing.\n",
        "\n",
        "---\n",
        "\n",
        "To run this, press \"Runtime\" and press \"Run all\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join our Discord if you need help!\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install \"unsloth[colab_ampere] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes etc\n",
        "* And Yi, Qwen ([llamafied](https://huggingface.co/models?sort=trending&search=qwen+llama)), Deepseek, all Llama, Mistral derived archs.\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HI68bWWbCfx7",
        "outputId": "ee997cb1-885c-4e45-c9a4-8e305ae802ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "382b428c-e6d1-4fbd-d1fb-263ab48469e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.2\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.1.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.22.post7. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-2-7b-bnb-4bit\", # \"unsloth/mistral-7b\" for 16bit loading\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "85b567d4-f6c2-4a44-94f2-2e45d0d68982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    max_seq_length = max_seq_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Tiny Stories dataset from https://huggingface.co/datasets/roneneldan/TinyStories. We only sample the first 5000 rows to speed training up. We must add `EOS_TOKEN` or `tokenizer.eos_token` or else the model's generation will go on forever.\n",
        "\n",
        "If you want to use the `ChatML` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    return example[\"text\"] + EOS_TOKEN"
      ],
      "metadata": {
        "id": "yXt8Na97yRe7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"text\", data_files=\"/content/drive/MyDrive/mr.txt\")\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset"
      ],
      "metadata": {
        "id": "pNaad0MwD_Zq",
        "outputId": "bb335a3f-eb8a-4ae4-f6d7-02d070918c1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "5536d7e4640649d4abb6ccf8afe0fd32",
            "5675cdd2a2f947c896eccc401573418f",
            "c475aed02e354bf8a992d214276b86ed",
            "b0e223c796074bd6a316c8fba77eb906",
            "301eeed534564b19b3c38ee3716ff3d1",
            "286dd99779754a35b3eb59e75118d604",
            "83baedf6e3994006a3b0de1d4a4e414c",
            "54839e7bc39a43d5847e0cedb66b981b",
            "7fc1a1a4c472499b97e70aa1bde50b01",
            "1592e7a87e8342b696108d2201362156",
            "17e78cc0ab79460e96df7097bc104c13"
          ]
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5536d7e4640649d4abb6ccf8afe0fd32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 33976000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset[\"train\"])"
      ],
      "metadata": {
        "id": "sPFZa-QEMWNw",
        "outputId": "2c6585aa-1920-4ded-cc3e-eec6a8062d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33976000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_iterator(batch_size=2500000):\n",
        "  for i in range(0, len(dataset[\"train\"]), batch_size):\n",
        "    yield dataset[\"train\"][i : i + batch_size]['text']"
      ],
      "metadata": {
        "id": "jT29hzI7IvGm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import SentencePieceBPETokenizer\n",
        "tokenizer = SentencePieceBPETokenizer()\n",
        "tokenizer.train_from_iterator(\n",
        "    iterator=batch_iterator(),\n",
        "    vocab_size=15000,\n",
        "    show_progress=True,\n",
        "    length=len(dataset[\"train\"])\n",
        ")"
      ],
      "metadata": {
        "id": "yOIkmsJ9KJE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_tok = \"marathi-tokenizer.json\"\n",
        "tokenizer.save(mr_tok, pretty=True)"
      ],
      "metadata": {
        "id": "OqBBaCZIZQGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"/content/drive/MyDrive/\"+mr_tok, pretty=True)"
      ],
      "metadata": {
        "id": "R7l-jbtsbZ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from transformers import LlamaTokenizer, TrainingArguments, AutoTokenizer,LlamaTokenizerFast\n",
        "\n",
        "\n",
        "\n",
        "# Load reference tokenizer\n",
        "\n",
        "reference_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-2-7b-bnb-4bit\")\n",
        "reference_tokenizer.save_pretrained(\"reference-tokenizer\")\n",
        "# Read and dump the json file for the new tokenizer and the reference tokenizer\n",
        "\n",
        "with open(mr_tok) as f:\n",
        "    new_llama_tokenizer_json = json.load(f)\n",
        "\n",
        "with open(\"reference-tokenizer/tokenizer.json\") as f:\n",
        "    reference_tokenizer_json = json.load(f)\n",
        "\n",
        "# Add the reference tokenizer's config to the new tokenizer's config\n",
        "new_llama_tokenizer_json[\"normalizer\"] = reference_tokenizer_json[\"normalizer\"]\n",
        "new_llama_tokenizer_json[\"pre_tokenizer\"] = reference_tokenizer_json[\"pre_tokenizer\"]\n",
        "new_llama_tokenizer_json[\"post_processor\"] = reference_tokenizer_json[\"post_processor\"]\n",
        "new_llama_tokenizer_json[\"decoder\"] = reference_tokenizer_json[\"decoder\"]\n",
        "new_llama_tokenizer_json[\"model\"]['fuse_unk'] = reference_tokenizer_json[\"model\"]['fuse_unk']\n",
        "new_llama_tokenizer_json[\"model\"]['byte_fallback'] = reference_tokenizer_json[\"model\"]['byte_fallback']\n",
        "\n",
        "# Dump the new tokenizer's config\n",
        "with open(mr_tok, \"w\") as f:\n",
        "    json.dump(new_llama_tokenizer_json, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "new_llama_tokenizer = LlamaTokenizerFast(\n",
        "    tokenizer_file=mr_tok,\n",
        "    unk_token=\"<unk>\",\n",
        "    unk_token_id=0,\n",
        "    bos_token=\"<s>\",\n",
        "    bos_token_id=1,\n",
        "    eos_token=\"</s>\",\n",
        "    eos_token_id=2,\n",
        "    pad_token=\"<pad>\",\n",
        "    pad_token_id=3,\n",
        "    padding_side=\"right\",\n",
        ")\n",
        "\n",
        "new_llama_tokenizer.save_pretrained(\"mr-llama-tokenizer\")"
      ],
      "metadata": {
        "id": "W8ZhTX3TqW75",
        "outputId": "fd0fad51-c843-42a1-bb61-c8d34ceaa639",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('mr-llama-tokenizer/tokenizer_config.json',\n",
              " 'mr-llama-tokenizer/special_tokens_map.json',\n",
              " 'mr-llama-tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = new_llama_tokenizer"
      ],
      "metadata": {
        "id": "_JTBbCvbuRJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 1 epoch to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_func(example):\n",
        "    return example[\"text\"] + EOS_TOKEN\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset['train'],\n",
        "    dataset_text_field = \"text\",\n",
        "    tokenizer = tokenizer,\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = True, # Packs short sequences together to save time!\n",
        "    formatting_func = formatting_func,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_ratio = 0.05,\n",
        "        max_grad_norm = 1.0,\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.1,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model!\n",
        "\n",
        "We first will try to see if the model follows the style and understands to write a story that is within the distribution of \"Tiny Stories\". Ie a story fit for a bed time story most likely.\n",
        "\n",
        "We select \"Once upon a time, in a galaxy, far far away,\" since it normally is associated with Star Wars."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "text_streamer = TextIteratorStreamer(tokenizer)\n",
        "import textwrap\n",
        "max_print_width = 100\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    \"Once upon a time, in a galaxy, far far away,\"\n",
        "]*1, return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 256,\n",
        "    use_cache = True,\n",
        ")\n",
        "thread = Thread(target = model.generate, kwargs = generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "length = 0\n",
        "for j, new_text in enumerate(text_streamer):\n",
        "    if j == 0:\n",
        "        wrapped_text = textwrap.wrap(new_text, width = max_print_width)\n",
        "        length = len(wrapped_text[-1])\n",
        "        wrapped_text = \"\\n\".join(wrapped_text)\n",
        "        print(wrapped_text, end = \"\")\n",
        "    else:\n",
        "        length += len(new_text)\n",
        "        if length >= max_print_width:\n",
        "            length = 0\n",
        "            print()\n",
        "        print(new_text, end = \"\")\n",
        "    pass\n",
        "pass"
      ],
      "metadata": {
        "id": "RHm11vaQRt1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see it's not about Star Wars, but more similar in style to Tiny Stories! Let's try it again, but by disabling our finetuned LoRA adapters. We now expect it to be about Star Wars."
      ],
      "metadata": {
        "id": "UB_GzeHXVAwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "text_streamer = TextIteratorStreamer(tokenizer)\n",
        "import textwrap\n",
        "max_print_width = 100\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    \"Once upon a time, in a galaxy, far far away,\"\n",
        "]*1, return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 256,\n",
        "    use_cache = True,\n",
        ")\n",
        "\n",
        "with model.disable_adapter(), torch.inference_mode():\n",
        "    thread = Thread(target = model.generate, kwargs = generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    length = 0\n",
        "    for j, new_text in enumerate(text_streamer):\n",
        "        if j == 0:\n",
        "            wrapped_text = textwrap.wrap(new_text, width = max_print_width)\n",
        "            length = len(wrapped_text[-1])\n",
        "            wrapped_text = \"\\n\".join(wrapped_text)\n",
        "            print(wrapped_text, end = \"\")\n",
        "        else:\n",
        "            length += len(new_text)\n",
        "            if length >= max_print_width:\n",
        "                length = 0\n",
        "                print()\n",
        "            print(new_text, end = \"\")\n",
        "        pass\n",
        "    pass"
      ],
      "metadata": {
        "id": "dZr6SsmDS1h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And it's about Star Wars!"
      ],
      "metadata": {
        "id": "TyaSSiI9VLNR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5536d7e4640649d4abb6ccf8afe0fd32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5675cdd2a2f947c896eccc401573418f",
              "IPY_MODEL_c475aed02e354bf8a992d214276b86ed",
              "IPY_MODEL_b0e223c796074bd6a316c8fba77eb906"
            ],
            "layout": "IPY_MODEL_301eeed534564b19b3c38ee3716ff3d1"
          }
        },
        "5675cdd2a2f947c896eccc401573418f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_286dd99779754a35b3eb59e75118d604",
            "placeholder": "​",
            "style": "IPY_MODEL_83baedf6e3994006a3b0de1d4a4e414c",
            "value": "Loading dataset shards: 100%"
          }
        },
        "c475aed02e354bf8a992d214276b86ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54839e7bc39a43d5847e0cedb66b981b",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fc1a1a4c472499b97e70aa1bde50b01",
            "value": 18
          }
        },
        "b0e223c796074bd6a316c8fba77eb906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1592e7a87e8342b696108d2201362156",
            "placeholder": "​",
            "style": "IPY_MODEL_17e78cc0ab79460e96df7097bc104c13",
            "value": " 18/18 [00:40&lt;00:00,  2.37s/it]"
          }
        },
        "301eeed534564b19b3c38ee3716ff3d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "286dd99779754a35b3eb59e75118d604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83baedf6e3994006a3b0de1d4a4e414c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54839e7bc39a43d5847e0cedb66b981b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc1a1a4c472499b97e70aa1bde50b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1592e7a87e8342b696108d2201362156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e78cc0ab79460e96df7097bc104c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}