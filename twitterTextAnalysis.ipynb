{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "import networkx as nx\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import os,stat\n",
    "\n",
    "\n",
    "CLUSTERS_OF_INTEREST = [36041,65124]\n",
    "MRH_FILE_PATH = \"pickles/mention_retweet_hastags(trending).pkl\"\n",
    "MRH_TIME_FILE_PATH = \"pickles/mention_retweet_hastags_timeobj(trending).pkl\"\n",
    "CLUSTER0_LINK_PATH = \"files/cluster36041.link\"\n",
    "CLUSTER1_LINK_PATH = \"files/cluster65124.link\"\n",
    "CLUSTER0_LINK_IMPORTANCE_PATH = \"files/cluster_36041(trending).urls.importance\"\n",
    "CLUSTER1_LINK_IMPORTANCE_PATH = \"files/cluster_65124(trending).urls.importance\"\n",
    "CLUSTER0_TEXT_IMPORTANCE_PATH = \"files/cluster_36041(trending).text.importance\"\n",
    "CLUSTER1_TEXT_IMPORTANCE_PATH = \"files/cluster_65124(trending).text.importance\"\n",
    "\n",
    "\n",
    "\n",
    "def pg_get_conn(database=\"fakenews\", user=\"fakenews\", password=\"fnd\"):\n",
    "    \"\"\"Get Postgres connection for fakenews\n",
    "\n",
    "    Returns:\n",
    "        Connection object : returns Post gres connection object\n",
    "\n",
    "    Args:\n",
    "        database (str, optional): Name of database\n",
    "        user (str, optional): Name of User\n",
    "        password (str, optional): Password of user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(database=database,\n",
    "                                user=user, password=password, host='localhost', port='5432')\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "def run_query(query=\"\"\"Select * from tweets_cleaned\"\"\", realDict = False, arg=None):\n",
    "    with pg_get_conn(database=\"abhishek\",user=\"abhishek\",password=\"vaishu\") as conn:\n",
    "        cur = conn.cursor(cursor_factory = psycopg2.extras.RealDictCursor) if realDict else conn.cursor()    \n",
    "        print(query) if not arg else print(cur.mogrify(query,(arg,)))\n",
    "        cur.execute(query) if not arg else cur.execute(query,(arg,))\n",
    "        try:\n",
    "            ans = cur.fetchall()\n",
    "        except psycopg2.ProgrammingError as e:\n",
    "            ans = None\n",
    "        return(ans)\n",
    "\n",
    "def create_graph(ls_tup):\n",
    "    G = nx.DiGraph()\n",
    "    for dc in tqdm(ls_tup):\n",
    "        if isinstance(ls_tup,dict):\n",
    "            tfrom=dc['tweet_from']\n",
    "            rt = dc['retweeted_status_user_handle']\n",
    "        else:\n",
    "            tfrom=dc[0]\n",
    "            rt=dc[1]\n",
    "        if G.has_edge(tfrom,rt):\n",
    "            G[tfrom][rt]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(tfrom,rt,weight=1)\n",
    "    return(G)\n",
    "def __custom_words_accumulator(series,limit=None):\n",
    "    c = Counter()\n",
    "    for sentence in series:\n",
    "        if sentence:\n",
    "            sent_list = sentence.split(\",\")\n",
    "            c.update(sent_list)\n",
    "    return c.most_common() if not limit else c.most_common(limit)\n",
    "\n",
    "def split_list(series,handleBool=True):\n",
    "    handles = []\n",
    "    listNoOfX = []\n",
    "    for groupList in series:\n",
    "        for handle,x in groupList:\n",
    "            handles.append(handle)\n",
    "            listNoOfX.append(x)\n",
    "    if handleBool :\n",
    "        return(handles)\n",
    "    else:\n",
    "        return(listNoOfX)\n",
    "        \n",
    "def get_barcharts(df,column_name=\"retweets\"):\n",
    "    wf = df.groupby(\"cluster\")[column_name].apply(__custom_words_accumulator,limit=50).reset_index()\n",
    "    wf2 = pd.DataFrame({\n",
    "    'cluster_id' : np.repeat(wf['cluster'],50),\n",
    "    'handle': split_list(wf[column_name]),\n",
    "    'noOfX': split_list(wf[column_name],handleBool=False)\n",
    "    })\n",
    "    clusters = wf2.cluster_id.unique()\n",
    "    sns.set(rc={'figure.figsize': (40,10)})\n",
    "    i = 0\n",
    "    f, ax = plt.subplots(len(clusters), 1, figsize=(40, 100))\n",
    "    f.tight_layout(pad=6.0)\n",
    "    for cid in clusters:\n",
    "        g = sns.barplot(x=\"handle\", y=\"noOfX\", hue=\"cluster_id\", data=wf2[wf2.cluster_id==cid],ax=ax[i])\n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=50, horizontalalignment='right')\n",
    "        i+=1    \n",
    "\n",
    "def plot_word_cloud(word_freq_dict,background_color=\"white\", width=800, height=1000,max_words=300, \n",
    "                    figsize=(50, 50), wc_only=False,color_map=\"viridis\"):\n",
    "    \"\"\"\n",
    "    Display the Word Cloud using Matplotlib\n",
    "    :param word_freq_dict: Dictionary of word frequencies\n",
    "    :type word_freq_dict: Dict\n",
    "    :return: None\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    word_cloud = WordCloud(background_color=background_color, width=width, height=height,\n",
    "                           max_words=max_words,colormap=color_map).generate_from_frequencies(frequencies=word_freq_dict)\n",
    "    if wc_only:\n",
    "        return word_cloud\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def reset_everything(source_table, min_degree=1,suffix=str(datetime.datetime.now())):\n",
    "    ls = run_query(\"Select tweet_from,retweeted_status_user_handle from {} where retweeted_status_user_handle is not null\".format(source_table))\n",
    "    G = create_graph(ls)\n",
    "    remove_nodes = [x[0] for x in G.degree(weight='weight') if x[1] <= min_degree]\n",
    "    G.remove_nodes_from(remove_nodes)\n",
    "    %reset_selective -f \"^ls$\"\n",
    "    %reset_selective -f \"^remove_node$\"\n",
    "    nx.write_gexf(G,\"graphs/G_{0}.gexf\".format(suffix))\n",
    "    update_MRH_PKL(suffix)\n",
    "    update_MRH_TIME_PKL(suffix)\n",
    "    write_info_to_file(source_table,suffix)\n",
    "    \n",
    "def write_info_to_file(tablename,suffix,text_columns=['text','urls']):\n",
    "    FILE_NAME=\"files/cluster_{}({}).{}.importance\"\n",
    "    for column in text_columns:\n",
    "        for cluster in CLUSTERS_OF_INTEREST:\n",
    "            try:\n",
    "                f = open(FILE_NAME.format(cluster,suffix,column),'w+')\n",
    "                absolute_path= os.path.realpath(f.name)\n",
    "                f.close()\n",
    "                os.chmod(absolute_path,stat.S_IRWXO |stat.S_IRWXG|stat.S_IRWXU)\n",
    "                query = \"COPY (SELECT t.{},c.importance from {} t JOIN cluster_mapping c ON t.tweet_from = c.id where c.cluster = {}) TO '{}';\".format(column,tablename,cluster,absolute_path)\n",
    "                run_query(query)\n",
    "            except PermissionError as e:\n",
    "                print(\"Please delete the file {}\".format(FILE_NAME.format(cluster,suffix,column)))\n",
    "#                 os.remove(FILE_NAME.format(cluster,suffix,column))\n",
    "                    \n",
    "def update_MRH_PKL(tablename=\"tweet_articles_tweepy\",suffix=str(datetime.datetime.now())):\n",
    "    ls = run_query(\"SELECT t.tweet_from,t.user_mentions_name,t.retweeted_status_user_handle,t.hashtags,c.cluster,c.importance FROM {} AS t INNER JOIN cluster_mapping AS c ON t.tweet_from = c.id WHERE c.cluster in %s\".format(tablename),arg=tuple(CLUSTERS_OF_INTEREST))\n",
    "    df = pd.DataFrame(ls,columns=['handle','mentions','retweets','hashtags','cluster','importance'])\n",
    "    for x in ['mentions','hashtags']:\n",
    "        df[x]=df[x].replace('{}',None)\n",
    "        df[x]=df[x].str.lstrip('{')\n",
    "        df[x]=df[x].str.rstrip('}')\n",
    "    global MRH_FILE_PATH\n",
    "    MRH_FILE_PATH = 'pickles/mention_retweet_hastags({0}).pkl'.format(suffix) \n",
    "    df.to_pickle(MRH_FILE_PATH)\n",
    "    return df\n",
    "\n",
    "def update_MRH_TIME_PKL(tablename=\"tweet_articles_tweepy\",suffix=str(datetime.datetime.now())):\n",
    "    ls = run_query(\"SELECT t.created_at,t.tweet_from,t.user_mentions_name,t.retweeted_status_user_handle,t.hashtags,c.cluster,c.importance FROM {} AS t INNER JOIN cluster_mapping AS c ON t.tweet_from = c.id WHERE c.cluster in %s\".format(tablename),arg=tuple(CLUSTERS_OF_INTEREST))\n",
    "    df = pd.DataFrame(ls ,columns = [\"time\",\"handle\",\"mentions\",\"retweets\",\"hashtags\",\"cluster\",\"importance\"])\n",
    "    df['time'] = pd.to_datetime(df['time'], format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "    for x in ['mentions','hashtags']:\n",
    "        df[x]=df[x].replace('{}',None)\n",
    "        df[x]=df[x].str.lstrip('{')\n",
    "        df[x]=df[x].str.rstrip('}')\n",
    "    global MRH_TIME_FILE_PATH\n",
    "    MRH_TIME_FILE_PATH = 'pickles/mention_retweet_hastags_timeobj({0}).pkl'.format(suffix) \n",
    "    df.to_pickle(MRH_TIME_FILE_PATH)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import repeat\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "cluster0_collector = []\n",
    "cluster1_collector = []\n",
    "def preprocess_text(line, cluster):\n",
    "    if not line.startswith(\"RT\"):\n",
    "        line = re.sub('#[a-zA-Z0-9_]*',\"<hashtag>\",line)\n",
    "        line = re.sub('(http|https)[a-zA-Z0-9://.]*',\"<link>\",line)\n",
    "        line = re.sub('@[a-zA-Z0-9_]*',\"<mention>\",line)\n",
    "        line = re.sub(r\"(\\\\n)*\",\"\",line)\n",
    "        line = re.sub('[0-9.]*$',\"\",line)\n",
    "        line = line.strip()\n",
    "        line = re.sub('<[link]*>$',\"\",line)\n",
    "        pattern = re.compile(r'<[a-z]*>', re.IGNORECASE)\n",
    "        match = pattern.findall(line)\n",
    "        if len(match) == len(line.split()):\n",
    "            return\n",
    "        if cluster == CLUSTERS_OF_INTEREST[0]:\n",
    "            cluster0_collector.append(line.strip())\n",
    "        else:\n",
    "            cluster1_collector.append(line.strip())\n",
    "\n",
    "def preprocess_multithreaded(filepath,cluster):\n",
    "    temp_ls=[]\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            temp_ls.append(line)\n",
    "            \n",
    "    with ThreadPoolExecutor(6) as executor:\n",
    "        executor.map(preprocess_text,temp_ls,repeat(cluster))\n",
    "    \n",
    "        \n",
    "    \n",
    "def get_cleaned_text(file_path):\n",
    "    ls_text=[]\n",
    "    with open(file_path,'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            if not line.startswith(\"RT\"):\n",
    "                line = re.sub('#[a-zA-Z0-9_]*',\"<hashtag>\",line)\n",
    "                line = re.sub('(http|https)[a-zA-Z0-9://.]*',\"<link>\",line)\n",
    "                line = re.sub('@[a-zA-Z0-9_]*',\"<mention>\",line)\n",
    "                line = re.sub(r\"(\\\\n)*\",\"\",line)\n",
    "                line = re.sub('[0-9.]*$',\"\",line)\n",
    "                line = line.strip()\n",
    "                line = re.sub('<[link]*>$',\"\",line)\n",
    "                pattern = re.compile(r'<[a-z]*>', re.IGNORECASE)\n",
    "                match = pattern.findall(line)\n",
    "                if len(match) == len(line.split()):\n",
    "                    continue\n",
    "                ls_text.append(line.strip())\n",
    "    return(ls_text)\n",
    "# cluster0_text = get_cleaned_text(CLUSTER0_TEXT_IMPORTANCE_PATH)\n",
    "# cluster1_text = get_cleaned_text(CLUSTER1_TEXT_IMPORTANCE_PATH)\n",
    "# preprocess_multithreaded(CLUSTER0_TEXT_IMPORTANCE_PATH,CLUSTERS_OF_INTEREST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c842ccdf3a8f4a9993fadc622599b06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d84eacebd144bb39a289ef7a603c6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cluster1_old = get_cleaned_text(\"files/cluster_65124(original).text.importance\")\n",
    "cluster0_old = get_cleaned_text(\"files/cluster_36041(original).text.importance\")\n",
    "cluster0_old.extend(cluster0_text)\n",
    "cluster1_old.extend(cluster1_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c6c75331e94c47981c793016b99baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7543229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"files/cluster0_aggregated_clean_text\",'w+') as f:\n",
    "    for line in tqdm(cluster0_old):\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sed -r 's/<hashtag>|<mention>|<link>//g' cluster0_aggregated_clean_text\n",
    "```\n",
    "Use the above command to replace all the mention hashtag and link tokens to generate a file only with text. This is useful for detecting language which we do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51ff3d3487b4ce6a4acfd6ae5cac929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7543229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb806883a00043029e581b595963ac90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3786061.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = {}\n",
    "for cluster in [0,1]:\n",
    "    text[cluster] = []\n",
    "    with open(\"files/cluster{}_aggregated_clean_text.no_tokens\".format(cluster),'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            text[cluster].append(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473 ms ± 5.01 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "%timeit for i in text[0][:100]:detect(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import re\n",
    "from itertools import repeat\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from langdetect.detector_factory import init_factory\n",
    "text_by_lang={}\n",
    "def classify_by_language(cluster,text):\n",
    "    pid = os.getpid()\n",
    "    lang = \"default\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    filename = \"files/lang_segregated_text/cluster_{}_{}.txt.{}\".format(cluster,lang,pid)\n",
    "    with open(filename,'a') as f:\n",
    "        f.write(text+\"\\n\")\n",
    "        \n",
    "for cluster in [0,1]:\n",
    "    with ProcessPoolExecutor(max_workers=6) as executor:\n",
    "        executor.map(classify_by_language,repeat(cluster),text[cluster],chunksize=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally understood why the memory module occupies so much memory\n",
    "refer [here](https://bugs.python.org/issue29842) to understand the issue, using chunksize should bind the usage of memory. Vanilla implementation holds all future object in memory until the task completes. Chunksize maps the data in chunks which helps release data regularly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0da2ccad30d41f687252edc666f2173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f081f4713ef491eaaa685b86c4cd5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dc014a552b4957a7fde56a8e87c19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d5816bed2b4ccc8c8044bb83bd5992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0570b2057d16401db097031e1d407a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf0d269fcc741988d10c0097e9aea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "clusters = [0,1]\n",
    "languages = ['en','hi','mr']\n",
    "dct = { 0 :{'en':[],'hi':[],'mr':[]},1:{'en':[],'hi':[],'mr':[]}}\n",
    "for cluster in clusters:\n",
    "    for language in languages:\n",
    "        files = glob(\"files/lang_segregated_text/cluster_{}_{}.txt.*\".format(cluster,language))\n",
    "        for file in tqdm(files):\n",
    "            with open(file,'r') as f:\n",
    "                dct[cluster][language].extend(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 en 8156964\n",
      "0 hi 3496804\n",
      "0 mr 259904\n",
      "1 en 4972100\n",
      "1 hi 1109984\n",
      "1 mr 164976\n"
     ]
    }
   ],
   "source": [
    "for cluster in dct.keys():\n",
    "    for language in dct[cluster].keys():\n",
    "        print(cluster,language,len(dct[cluster][language]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(dct,open('pickles/segregated_text_dct.pkl','wb+'),protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
